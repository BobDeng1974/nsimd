<!--

Copyright (c) 2019 Agenium Scale

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

-->

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NSIMD documentation</title>
    <style type="text/css">
      body {
        /*margin:40px auto;*/
        margin:10px auto;
        /*max-width:650px;*/
        max-width:800px;
        /*line-height:1.6;*/
        line-height:1.4;
        /*font-size:18px;*/
        color:#444;
        padding:0 10px
      }
      h1,h2,h3 {
        line-height:1.2
      }
      table,th, td {
        border: 1px solid gray;
        border-collapse : collapse;
        padding: 1px 3px;
      }
    </style>
    <!-- https://www.mathjax.org/#gettingstarted -->
    <script src="assets/polyfill.min.js"></script>
    <script id="MathJax-script" async src="assets/tex-mml-chtml.js"></script>
    <!-- Highlight.js -->
    <link rel="stylesheet" href= "assets/highlight.js.default.min.css">
    <script src="assets/highlight.min.js"></script>
    <script src="assets/cpp.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
<body>

<center>
  <img src="img/logo.svg"><br>
  <br>
  <a href="index.html">Index</a> |
  <a href="quick_start.html">Quick Start</a> |
  <a href="tutorials.html">Tutorials</a> |
  <a href="faq.html">FAQ</a> |
  <a href="contribute.html">Contribute</a> |
  <a href="overview.html">API overview</a> |
  <a href="api.html">API reference</a>
</center>
<h1>Load array of structure</h1>
<h2>Description</h2>
<p>Load array of structures of 2 members from unaligned memory.</p>
<h2>C base API (generic)</h2>
<pre class="c"><code>#define vload2u(a0, type)
#define vload2u_e(a0, type, simd_ext)</code></pre>
<h2>C++ base API (generic)</h2>
<pre class="c++"><code>template &lt;typename A0, typename T&gt; typename simd_traits&lt;T, NSIMD_SIMD&gt;::simd_vectorx2 load2u(A0 a0, T);</code></pre>
<h2>C++ advanced API</h2>
<pre class="c++"><code>template &lt;typename T, typename SimdExt, typename A0&gt; packx2&lt;T, 1, SimdExt&gt; load2u(packx2&lt;T, 1, SimdExt&gt; const&amp;, A0 a0);
template &lt;typename T, int N, typename SimdExt, typename A0&gt; packx2&lt;T, N, SimdExt&gt; load2u(packx2&lt;T, N, SimdExt&gt; const&amp;, A0 a0);
template &lt;typename SimdVector, typename A0&gt; SimdVector load2u(A0 a0);</code></pre>
<h2>C base API (architecture specifics)</h2>
<h3>AARCH64</h3>
<pre class="c"><code>nsimd_aarch64_vf64x2 nsimd_load2u_aarch64_f64(f64 const* a0);
nsimd_aarch64_vf32x2 nsimd_load2u_aarch64_f32(f32 const* a0);
nsimd_aarch64_vf16x2 nsimd_load2u_aarch64_f16(f16 const* a0);
nsimd_aarch64_vi64x2 nsimd_load2u_aarch64_i64(i64 const* a0);
nsimd_aarch64_vi32x2 nsimd_load2u_aarch64_i32(i32 const* a0);
nsimd_aarch64_vi16x2 nsimd_load2u_aarch64_i16(i16 const* a0);
nsimd_aarch64_vi8x2 nsimd_load2u_aarch64_i8(i8 const* a0);
nsimd_aarch64_vu64x2 nsimd_load2u_aarch64_u64(u64 const* a0);
nsimd_aarch64_vu32x2 nsimd_load2u_aarch64_u32(u32 const* a0);
nsimd_aarch64_vu16x2 nsimd_load2u_aarch64_u16(u16 const* a0);
nsimd_aarch64_vu8x2 nsimd_load2u_aarch64_u8(u8 const* a0);</code></pre>
<h3>SSE2</h3>
<pre class="c"><code>nsimd_sse2_vf64x2 nsimd_load2u_sse2_f64(f64 const* a0);
nsimd_sse2_vf32x2 nsimd_load2u_sse2_f32(f32 const* a0);
nsimd_sse2_vf16x2 nsimd_load2u_sse2_f16(f16 const* a0);
nsimd_sse2_vi64x2 nsimd_load2u_sse2_i64(i64 const* a0);
nsimd_sse2_vi32x2 nsimd_load2u_sse2_i32(i32 const* a0);
nsimd_sse2_vi16x2 nsimd_load2u_sse2_i16(i16 const* a0);
nsimd_sse2_vi8x2 nsimd_load2u_sse2_i8(i8 const* a0);
nsimd_sse2_vu64x2 nsimd_load2u_sse2_u64(u64 const* a0);
nsimd_sse2_vu32x2 nsimd_load2u_sse2_u32(u32 const* a0);
nsimd_sse2_vu16x2 nsimd_load2u_sse2_u16(u16 const* a0);
nsimd_sse2_vu8x2 nsimd_load2u_sse2_u8(u8 const* a0);</code></pre>
<h3>AVX512_SKYLAKE</h3>
<pre class="c"><code>nsimd_avx512_skylake_vf64x2 nsimd_load2u_avx512_skylake_f64(f64 const* a0);
nsimd_avx512_skylake_vf32x2 nsimd_load2u_avx512_skylake_f32(f32 const* a0);
nsimd_avx512_skylake_vf16x2 nsimd_load2u_avx512_skylake_f16(f16 const* a0);
nsimd_avx512_skylake_vi64x2 nsimd_load2u_avx512_skylake_i64(i64 const* a0);
nsimd_avx512_skylake_vi32x2 nsimd_load2u_avx512_skylake_i32(i32 const* a0);
nsimd_avx512_skylake_vi16x2 nsimd_load2u_avx512_skylake_i16(i16 const* a0);
nsimd_avx512_skylake_vi8x2 nsimd_load2u_avx512_skylake_i8(i8 const* a0);
nsimd_avx512_skylake_vu64x2 nsimd_load2u_avx512_skylake_u64(u64 const* a0);
nsimd_avx512_skylake_vu32x2 nsimd_load2u_avx512_skylake_u32(u32 const* a0);
nsimd_avx512_skylake_vu16x2 nsimd_load2u_avx512_skylake_u16(u16 const* a0);
nsimd_avx512_skylake_vu8x2 nsimd_load2u_avx512_skylake_u8(u8 const* a0);</code></pre>
<h3>AVX</h3>
<pre class="c"><code>nsimd_avx_vf64x2 nsimd_load2u_avx_f64(f64 const* a0);
nsimd_avx_vf32x2 nsimd_load2u_avx_f32(f32 const* a0);
nsimd_avx_vf16x2 nsimd_load2u_avx_f16(f16 const* a0);
nsimd_avx_vi64x2 nsimd_load2u_avx_i64(i64 const* a0);
nsimd_avx_vi32x2 nsimd_load2u_avx_i32(i32 const* a0);
nsimd_avx_vi16x2 nsimd_load2u_avx_i16(i16 const* a0);
nsimd_avx_vi8x2 nsimd_load2u_avx_i8(i8 const* a0);
nsimd_avx_vu64x2 nsimd_load2u_avx_u64(u64 const* a0);
nsimd_avx_vu32x2 nsimd_load2u_avx_u32(u32 const* a0);
nsimd_avx_vu16x2 nsimd_load2u_avx_u16(u16 const* a0);
nsimd_avx_vu8x2 nsimd_load2u_avx_u8(u8 const* a0);</code></pre>
<h3>AVX2</h3>
<pre class="c"><code>nsimd_avx2_vf64x2 nsimd_load2u_avx2_f64(f64 const* a0);
nsimd_avx2_vf32x2 nsimd_load2u_avx2_f32(f32 const* a0);
nsimd_avx2_vf16x2 nsimd_load2u_avx2_f16(f16 const* a0);
nsimd_avx2_vi64x2 nsimd_load2u_avx2_i64(i64 const* a0);
nsimd_avx2_vi32x2 nsimd_load2u_avx2_i32(i32 const* a0);
nsimd_avx2_vi16x2 nsimd_load2u_avx2_i16(i16 const* a0);
nsimd_avx2_vi8x2 nsimd_load2u_avx2_i8(i8 const* a0);
nsimd_avx2_vu64x2 nsimd_load2u_avx2_u64(u64 const* a0);
nsimd_avx2_vu32x2 nsimd_load2u_avx2_u32(u32 const* a0);
nsimd_avx2_vu16x2 nsimd_load2u_avx2_u16(u16 const* a0);
nsimd_avx2_vu8x2 nsimd_load2u_avx2_u8(u8 const* a0);</code></pre>
<h3>SVE</h3>
<pre class="c"><code>nsimd_sve_vf64x2 nsimd_load2u_sve_f64(f64 const* a0);
nsimd_sve_vf32x2 nsimd_load2u_sve_f32(f32 const* a0);
nsimd_sve_vf16x2 nsimd_load2u_sve_f16(f16 const* a0);
nsimd_sve_vi64x2 nsimd_load2u_sve_i64(i64 const* a0);
nsimd_sve_vi32x2 nsimd_load2u_sve_i32(i32 const* a0);
nsimd_sve_vi16x2 nsimd_load2u_sve_i16(i16 const* a0);
nsimd_sve_vi8x2 nsimd_load2u_sve_i8(i8 const* a0);
nsimd_sve_vu64x2 nsimd_load2u_sve_u64(u64 const* a0);
nsimd_sve_vu32x2 nsimd_load2u_sve_u32(u32 const* a0);
nsimd_sve_vu16x2 nsimd_load2u_sve_u16(u16 const* a0);
nsimd_sve_vu8x2 nsimd_load2u_sve_u8(u8 const* a0);</code></pre>
<h3>CPU</h3>
<pre class="c"><code>nsimd_cpu_vf64x2 nsimd_load2u_cpu_f64(f64 const* a0);
nsimd_cpu_vf32x2 nsimd_load2u_cpu_f32(f32 const* a0);
nsimd_cpu_vf16x2 nsimd_load2u_cpu_f16(f16 const* a0);
nsimd_cpu_vi64x2 nsimd_load2u_cpu_i64(i64 const* a0);
nsimd_cpu_vi32x2 nsimd_load2u_cpu_i32(i32 const* a0);
nsimd_cpu_vi16x2 nsimd_load2u_cpu_i16(i16 const* a0);
nsimd_cpu_vi8x2 nsimd_load2u_cpu_i8(i8 const* a0);
nsimd_cpu_vu64x2 nsimd_load2u_cpu_u64(u64 const* a0);
nsimd_cpu_vu32x2 nsimd_load2u_cpu_u32(u32 const* a0);
nsimd_cpu_vu16x2 nsimd_load2u_cpu_u16(u16 const* a0);
nsimd_cpu_vu8x2 nsimd_load2u_cpu_u8(u8 const* a0);</code></pre>
<h3>AVX512_KNL</h3>
<pre class="c"><code>nsimd_avx512_knl_vf64x2 nsimd_load2u_avx512_knl_f64(f64 const* a0);
nsimd_avx512_knl_vf32x2 nsimd_load2u_avx512_knl_f32(f32 const* a0);
nsimd_avx512_knl_vf16x2 nsimd_load2u_avx512_knl_f16(f16 const* a0);
nsimd_avx512_knl_vi64x2 nsimd_load2u_avx512_knl_i64(i64 const* a0);
nsimd_avx512_knl_vi32x2 nsimd_load2u_avx512_knl_i32(i32 const* a0);
nsimd_avx512_knl_vi16x2 nsimd_load2u_avx512_knl_i16(i16 const* a0);
nsimd_avx512_knl_vi8x2 nsimd_load2u_avx512_knl_i8(i8 const* a0);
nsimd_avx512_knl_vu64x2 nsimd_load2u_avx512_knl_u64(u64 const* a0);
nsimd_avx512_knl_vu32x2 nsimd_load2u_avx512_knl_u32(u32 const* a0);
nsimd_avx512_knl_vu16x2 nsimd_load2u_avx512_knl_u16(u16 const* a0);
nsimd_avx512_knl_vu8x2 nsimd_load2u_avx512_knl_u8(u8 const* a0);</code></pre>
<h3>NEON128</h3>
<pre class="c"><code>nsimd_neon128_vf64x2 nsimd_load2u_neon128_f64(f64 const* a0);
nsimd_neon128_vf32x2 nsimd_load2u_neon128_f32(f32 const* a0);
nsimd_neon128_vf16x2 nsimd_load2u_neon128_f16(f16 const* a0);
nsimd_neon128_vi64x2 nsimd_load2u_neon128_i64(i64 const* a0);
nsimd_neon128_vi32x2 nsimd_load2u_neon128_i32(i32 const* a0);
nsimd_neon128_vi16x2 nsimd_load2u_neon128_i16(i16 const* a0);
nsimd_neon128_vi8x2 nsimd_load2u_neon128_i8(i8 const* a0);
nsimd_neon128_vu64x2 nsimd_load2u_neon128_u64(u64 const* a0);
nsimd_neon128_vu32x2 nsimd_load2u_neon128_u32(u32 const* a0);
nsimd_neon128_vu16x2 nsimd_load2u_neon128_u16(u16 const* a0);
nsimd_neon128_vu8x2 nsimd_load2u_neon128_u8(u8 const* a0);</code></pre>
<h3>SSE42</h3>
<pre class="c"><code>nsimd_sse42_vf64x2 nsimd_load2u_sse42_f64(f64 const* a0);
nsimd_sse42_vf32x2 nsimd_load2u_sse42_f32(f32 const* a0);
nsimd_sse42_vf16x2 nsimd_load2u_sse42_f16(f16 const* a0);
nsimd_sse42_vi64x2 nsimd_load2u_sse42_i64(i64 const* a0);
nsimd_sse42_vi32x2 nsimd_load2u_sse42_i32(i32 const* a0);
nsimd_sse42_vi16x2 nsimd_load2u_sse42_i16(i16 const* a0);
nsimd_sse42_vi8x2 nsimd_load2u_sse42_i8(i8 const* a0);
nsimd_sse42_vu64x2 nsimd_load2u_sse42_u64(u64 const* a0);
nsimd_sse42_vu32x2 nsimd_load2u_sse42_u32(u32 const* a0);
nsimd_sse42_vu16x2 nsimd_load2u_sse42_u16(u16 const* a0);
nsimd_sse42_vu8x2 nsimd_load2u_sse42_u8(u8 const* a0);</code></pre>
<h2>C++ base API (architecture specifics)</h2>
<h3>AARCH64</h3>
<pre class="c"><code>nsimd_aarch64_vf64x2 load2u(f64 const* a0, f64, aarch64);
nsimd_aarch64_vf32x2 load2u(f32 const* a0, f32, aarch64);
nsimd_aarch64_vf16x2 load2u(f16 const* a0, f16, aarch64);
nsimd_aarch64_vi64x2 load2u(i64 const* a0, i64, aarch64);
nsimd_aarch64_vi32x2 load2u(i32 const* a0, i32, aarch64);
nsimd_aarch64_vi16x2 load2u(i16 const* a0, i16, aarch64);
nsimd_aarch64_vi8x2 load2u(i8 const* a0, i8, aarch64);
nsimd_aarch64_vu64x2 load2u(u64 const* a0, u64, aarch64);
nsimd_aarch64_vu32x2 load2u(u32 const* a0, u32, aarch64);
nsimd_aarch64_vu16x2 load2u(u16 const* a0, u16, aarch64);
nsimd_aarch64_vu8x2 load2u(u8 const* a0, u8, aarch64);</code></pre>
<h3>SSE2</h3>
<pre class="c"><code>nsimd_sse2_vf64x2 load2u(f64 const* a0, f64, sse2);
nsimd_sse2_vf32x2 load2u(f32 const* a0, f32, sse2);
nsimd_sse2_vf16x2 load2u(f16 const* a0, f16, sse2);
nsimd_sse2_vi64x2 load2u(i64 const* a0, i64, sse2);
nsimd_sse2_vi32x2 load2u(i32 const* a0, i32, sse2);
nsimd_sse2_vi16x2 load2u(i16 const* a0, i16, sse2);
nsimd_sse2_vi8x2 load2u(i8 const* a0, i8, sse2);
nsimd_sse2_vu64x2 load2u(u64 const* a0, u64, sse2);
nsimd_sse2_vu32x2 load2u(u32 const* a0, u32, sse2);
nsimd_sse2_vu16x2 load2u(u16 const* a0, u16, sse2);
nsimd_sse2_vu8x2 load2u(u8 const* a0, u8, sse2);</code></pre>
<h3>AVX512_SKYLAKE</h3>
<pre class="c"><code>nsimd_avx512_skylake_vf64x2 load2u(f64 const* a0, f64, avx512_skylake);
nsimd_avx512_skylake_vf32x2 load2u(f32 const* a0, f32, avx512_skylake);
nsimd_avx512_skylake_vf16x2 load2u(f16 const* a0, f16, avx512_skylake);
nsimd_avx512_skylake_vi64x2 load2u(i64 const* a0, i64, avx512_skylake);
nsimd_avx512_skylake_vi32x2 load2u(i32 const* a0, i32, avx512_skylake);
nsimd_avx512_skylake_vi16x2 load2u(i16 const* a0, i16, avx512_skylake);
nsimd_avx512_skylake_vi8x2 load2u(i8 const* a0, i8, avx512_skylake);
nsimd_avx512_skylake_vu64x2 load2u(u64 const* a0, u64, avx512_skylake);
nsimd_avx512_skylake_vu32x2 load2u(u32 const* a0, u32, avx512_skylake);
nsimd_avx512_skylake_vu16x2 load2u(u16 const* a0, u16, avx512_skylake);
nsimd_avx512_skylake_vu8x2 load2u(u8 const* a0, u8, avx512_skylake);</code></pre>
<h3>AVX</h3>
<pre class="c"><code>nsimd_avx_vf64x2 load2u(f64 const* a0, f64, avx);
nsimd_avx_vf32x2 load2u(f32 const* a0, f32, avx);
nsimd_avx_vf16x2 load2u(f16 const* a0, f16, avx);
nsimd_avx_vi64x2 load2u(i64 const* a0, i64, avx);
nsimd_avx_vi32x2 load2u(i32 const* a0, i32, avx);
nsimd_avx_vi16x2 load2u(i16 const* a0, i16, avx);
nsimd_avx_vi8x2 load2u(i8 const* a0, i8, avx);
nsimd_avx_vu64x2 load2u(u64 const* a0, u64, avx);
nsimd_avx_vu32x2 load2u(u32 const* a0, u32, avx);
nsimd_avx_vu16x2 load2u(u16 const* a0, u16, avx);
nsimd_avx_vu8x2 load2u(u8 const* a0, u8, avx);</code></pre>
<h3>AVX2</h3>
<pre class="c"><code>nsimd_avx2_vf64x2 load2u(f64 const* a0, f64, avx2);
nsimd_avx2_vf32x2 load2u(f32 const* a0, f32, avx2);
nsimd_avx2_vf16x2 load2u(f16 const* a0, f16, avx2);
nsimd_avx2_vi64x2 load2u(i64 const* a0, i64, avx2);
nsimd_avx2_vi32x2 load2u(i32 const* a0, i32, avx2);
nsimd_avx2_vi16x2 load2u(i16 const* a0, i16, avx2);
nsimd_avx2_vi8x2 load2u(i8 const* a0, i8, avx2);
nsimd_avx2_vu64x2 load2u(u64 const* a0, u64, avx2);
nsimd_avx2_vu32x2 load2u(u32 const* a0, u32, avx2);
nsimd_avx2_vu16x2 load2u(u16 const* a0, u16, avx2);
nsimd_avx2_vu8x2 load2u(u8 const* a0, u8, avx2);</code></pre>
<h3>SVE</h3>
<pre class="c"><code>nsimd_sve_vf64x2 load2u(f64 const* a0, f64, sve);
nsimd_sve_vf32x2 load2u(f32 const* a0, f32, sve);
nsimd_sve_vf16x2 load2u(f16 const* a0, f16, sve);
nsimd_sve_vi64x2 load2u(i64 const* a0, i64, sve);
nsimd_sve_vi32x2 load2u(i32 const* a0, i32, sve);
nsimd_sve_vi16x2 load2u(i16 const* a0, i16, sve);
nsimd_sve_vi8x2 load2u(i8 const* a0, i8, sve);
nsimd_sve_vu64x2 load2u(u64 const* a0, u64, sve);
nsimd_sve_vu32x2 load2u(u32 const* a0, u32, sve);
nsimd_sve_vu16x2 load2u(u16 const* a0, u16, sve);
nsimd_sve_vu8x2 load2u(u8 const* a0, u8, sve);</code></pre>
<h3>CPU</h3>
<pre class="c"><code>nsimd_cpu_vf64x2 load2u(f64 const* a0, f64, cpu);
nsimd_cpu_vf32x2 load2u(f32 const* a0, f32, cpu);
nsimd_cpu_vf16x2 load2u(f16 const* a0, f16, cpu);
nsimd_cpu_vi64x2 load2u(i64 const* a0, i64, cpu);
nsimd_cpu_vi32x2 load2u(i32 const* a0, i32, cpu);
nsimd_cpu_vi16x2 load2u(i16 const* a0, i16, cpu);
nsimd_cpu_vi8x2 load2u(i8 const* a0, i8, cpu);
nsimd_cpu_vu64x2 load2u(u64 const* a0, u64, cpu);
nsimd_cpu_vu32x2 load2u(u32 const* a0, u32, cpu);
nsimd_cpu_vu16x2 load2u(u16 const* a0, u16, cpu);
nsimd_cpu_vu8x2 load2u(u8 const* a0, u8, cpu);</code></pre>
<h3>AVX512_KNL</h3>
<pre class="c"><code>nsimd_avx512_knl_vf64x2 load2u(f64 const* a0, f64, avx512_knl);
nsimd_avx512_knl_vf32x2 load2u(f32 const* a0, f32, avx512_knl);
nsimd_avx512_knl_vf16x2 load2u(f16 const* a0, f16, avx512_knl);
nsimd_avx512_knl_vi64x2 load2u(i64 const* a0, i64, avx512_knl);
nsimd_avx512_knl_vi32x2 load2u(i32 const* a0, i32, avx512_knl);
nsimd_avx512_knl_vi16x2 load2u(i16 const* a0, i16, avx512_knl);
nsimd_avx512_knl_vi8x2 load2u(i8 const* a0, i8, avx512_knl);
nsimd_avx512_knl_vu64x2 load2u(u64 const* a0, u64, avx512_knl);
nsimd_avx512_knl_vu32x2 load2u(u32 const* a0, u32, avx512_knl);
nsimd_avx512_knl_vu16x2 load2u(u16 const* a0, u16, avx512_knl);
nsimd_avx512_knl_vu8x2 load2u(u8 const* a0, u8, avx512_knl);</code></pre>
<h3>NEON128</h3>
<pre class="c"><code>nsimd_neon128_vf64x2 load2u(f64 const* a0, f64, neon128);
nsimd_neon128_vf32x2 load2u(f32 const* a0, f32, neon128);
nsimd_neon128_vf16x2 load2u(f16 const* a0, f16, neon128);
nsimd_neon128_vi64x2 load2u(i64 const* a0, i64, neon128);
nsimd_neon128_vi32x2 load2u(i32 const* a0, i32, neon128);
nsimd_neon128_vi16x2 load2u(i16 const* a0, i16, neon128);
nsimd_neon128_vi8x2 load2u(i8 const* a0, i8, neon128);
nsimd_neon128_vu64x2 load2u(u64 const* a0, u64, neon128);
nsimd_neon128_vu32x2 load2u(u32 const* a0, u32, neon128);
nsimd_neon128_vu16x2 load2u(u16 const* a0, u16, neon128);
nsimd_neon128_vu8x2 load2u(u8 const* a0, u8, neon128);</code></pre>
<h3>SSE42</h3>
<pre class="c"><code>nsimd_sse42_vf64x2 load2u(f64 const* a0, f64, sse42);
nsimd_sse42_vf32x2 load2u(f32 const* a0, f32, sse42);
nsimd_sse42_vf16x2 load2u(f16 const* a0, f16, sse42);
nsimd_sse42_vi64x2 load2u(i64 const* a0, i64, sse42);
nsimd_sse42_vi32x2 load2u(i32 const* a0, i32, sse42);
nsimd_sse42_vi16x2 load2u(i16 const* a0, i16, sse42);
nsimd_sse42_vi8x2 load2u(i8 const* a0, i8, sse42);
nsimd_sse42_vu64x2 load2u(u64 const* a0, u64, sse42);
nsimd_sse42_vu32x2 load2u(u32 const* a0, u32, sse42);
nsimd_sse42_vu16x2 load2u(u16 const* a0, u16, sse42);
nsimd_sse42_vu8x2 load2u(u8 const* a0, u8, sse42);</code></pre>
  </body>
</html>
